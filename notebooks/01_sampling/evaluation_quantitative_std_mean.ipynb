{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84810ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib agg\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import visualization\n",
    "from utils import results, experiments, settings, equioutput, evaluation\n",
    "import os\n",
    "import global_settings\n",
    "from data import datasets\n",
    "from numpyro import distributions\n",
    "import transformations\n",
    "import flax.linen as nn\n",
    "from utils import conversion\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import models\n",
    "import laplace\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "rng_key, rng_key_ = jax.random.split(jax.random.PRNGKey(0))\n",
    "\n",
    "method = \"de\"\n",
    "arch = \"1x3\"\n",
    "#arch = \"3x16\"\n",
    "all_datasets = global_settings.DATASET_NAMES_TOY + global_settings.DATASET_NAMES_BENCHMARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computed_lppd_mcmc(inputs, outputs, parameters_network, parameters_data_std, regression_model):\n",
    "    \"\"\"variant of lppd for mcmc samples\"\"\"\n",
    "    log_prob_means = []\n",
    "    for xi, yi in zip(inputs, outputs):\n",
    "        yi_preds = jax.vmap(regression_model._transformation.apply_from_vector, in_axes=(None, 0))(xi, parameters_network)\n",
    "        prob_mean = jnp.exp(regression_model._outputs_likelihood(\n",
    "            yi_preds,\n",
    "            parameters_data_std).log_prob(yi)).mean(axis=0)\n",
    "        log_prob_means.append(jnp.log(prob_mean))\n",
    "    log_prob_means = jnp.array(log_prob_means)\n",
    "    return log_prob_means\n",
    "\n",
    "def computed_lppd_la(inputs, outputs, parameters_network, parameters_data_std, regression_model):\n",
    "    log_prob_means = []\n",
    "    for xi, yi in zip(inputs, outputs):\n",
    "        yi_preds = jax.vmap(regression_model._transformation.apply_from_vector, in_axes=(None, 0))(xi, parameters_network)\n",
    "        yi_std = parameters_data_std\n",
    "        prob_mean = jnp.exp(regression_model._outputs_likelihood(\n",
    "            yi_preds,\n",
    "            yi_std).log_prob(yi)).mean(axis=0)\n",
    "        if prob_mean > 0.0:\n",
    "            log_prob_means.append(jnp.log(prob_mean))\n",
    "    log_prob_means = jnp.array(log_prob_means)\n",
    "    return log_prob_means\n",
    "\n",
    "def computed_lppd_de(inputs, outputs, parameters_network, parameters_data_std, regression_model, mean_y, std_y):\n",
    "    log_prob_means = []\n",
    "    for xi, yi in zip(inputs, outputs):\n",
    "        yi_preds_scaled = jax.vmap(regression_model._transformation.apply_from_vector, in_axes=(None, 0))(xi, parameters_network)\n",
    "        yi_preds = (yi_preds_scaled * std_y) + mean_y\n",
    "        \n",
    "        mean = yi_preds.mean(0)\n",
    "        variance = (jnp.power(parameters_data_std, 2) + jnp.power(yi_preds, 2)).mean(0) - jnp.power(mean, 2)\n",
    "        std = jnp.power(variance, 0.5)\n",
    "        \n",
    "        predictive_prob = jnp.exp(distributions.Normal(mean, std).log_prob(yi))\n",
    "        if predictive_prob > 0.0:\n",
    "            log_prob_means.append(jnp.log(predictive_prob))\n",
    "    log_prob_means = jnp.array(log_prob_means)\n",
    "    return log_prob_means\n",
    "\n",
    "def load_dataset(name):\n",
    "    if name in global_settings.DATASET_NAMES_TOY:\n",
    "        with open(os.path.join(global_settings.PATH_DATASETS, \"toy_dataset_indices_0.2.json\"), 'r') as f:\n",
    "            indices = json.load(f)\n",
    "            split = {\n",
    "                \"data_train\": indices[name][\"train\"],\n",
    "                \"data_validate\": [],\n",
    "                \"data_test\": indices[name][\"validate\"] # validate as test data, since we do not need validation data...\n",
    "            }\n",
    "    elif name in global_settings.DATASET_NAMES_BENCHMARK:\n",
    "        with open(os.path.join(os.path.join(global_settings.PATH_DATASETS, \"benchmark_data\"), \"dataset_indices_0.2.json\"), 'r') as f:\n",
    "            indices = json.load(f)\n",
    "            split = {\n",
    "                \"data_train\": indices[name][\"train\"],\n",
    "                \"data_validate\": [],\n",
    "                \"data_test\": indices[name][\"validate\"] # validate as test data, since we do not need validation data...\n",
    "            }\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    # load dataset\n",
    "    if name == \"izmailov\":\n",
    "        dataset = datasets.Izmailov(split=split)\n",
    "    elif name == \"sinusoidal\":\n",
    "        dataset = datasets.Sinusoidal(split=split)\n",
    "    elif name == \"regression2d\":\n",
    "        dataset = datasets.Regression2d(split=split)\n",
    "    elif name in global_settings.DATASET_NAMES_BENCHMARK:\n",
    "        dataset = datasets.GenericBenchmark(dataset_name=name, split=split)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac99a2f",
   "metadata": {},
   "source": [
    "# point estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f46871",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sinusoidal\")\n",
    "print(dataset.data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ddb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_0 = f\"/home/gw/data/experiments/paper/results/mu_sigma.json\"\n",
    "with open(file_path_0, 'r') as f:\n",
    "    mu_sigma = json.load(f)\n",
    "print(mu_sigma[\"sinusoidal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030393b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dataset_name in all_datasets[:]:\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    input_dim = len(dataset.conditional_indices)\n",
    "    output_dim = len(dataset.dependent_indices)\n",
    "    \n",
    "    if arch == \"1x3\":\n",
    "        model_transformation = transformations.Sequential([\n",
    "            nn.Dense(3),\n",
    "            nn.tanh,\n",
    "            nn.Dense(1)\n",
    "        ])\n",
    "        model_transformation_la = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 3),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(3, output_dim)\n",
    "        )\n",
    "    elif arch == \"3x16\":\n",
    "        model_transformation = transformations.Sequential([\n",
    "            nn.Dense(16),\n",
    "            nn.tanh,\n",
    "            nn.Dense(16),\n",
    "            nn.tanh,\n",
    "            nn.Dense(16),\n",
    "            nn.tanh,\n",
    "            nn.Dense(1)\n",
    "        ])\n",
    "        model_transformation_la = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 16),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(16, 16),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(16, 16),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(16, output_dim)\n",
    "        )\n",
    "\n",
    "    file_path_1 = f\"/home/gw/data/experiments/paper/results/weights_{arch}/weights_{method}_mlp_{arch}_{dataset_name}.json\"\n",
    "    file_path_2 = f\"/home/gw/data/experiments/paper/results/weights_{arch}/sigmas_{method}_mlp_{arch}_{dataset_name}.json\"\n",
    "    with open(file_path_1, 'r') as f:\n",
    "        samples_dict = json.load(f)\n",
    "    with open(file_path_2, 'r') as f:\n",
    "        sigmas_dict = json.load(f)\n",
    "\n",
    "    samples = []\n",
    "    sigmas = []\n",
    "    for key in samples_dict.keys():\n",
    "        sample = samples_dict[key]\n",
    "        sigma = sigmas_dict[key]\n",
    "        samples.append(sample)\n",
    "        sigmas.append(sigma)\n",
    "    samples = np.asarray(samples)\n",
    "    sigmas = np.asarray(sigmas).reshape((-1, 1))\n",
    "    \n",
    "    inputs = dataset.data[0, dataset.conditional_indices]\n",
    "    template = model_transformation.init(rng_key, inputs)\n",
    "    samples_converted = []\n",
    "    for sample in samples:\n",
    "        sample_converted = conversion.torch_parameters_vector_to_flax_parameters_vector(torch.tensor(sample), template)\n",
    "        samples_converted.append(sample_converted)\n",
    "    samples_converted = jnp.array(samples_converted)\n",
    "\n",
    "    model = models.Regression(\n",
    "        transformation=model_transformation,\n",
    "        dataset=dataset\n",
    "    )\n",
    "    \n",
    "    mean_x = jnp.array(mu_sigma[dataset_name][\"mean_x\"])\n",
    "    std_x = jnp.sqrt(jnp.array(mu_sigma[dataset_name][\"var_x\"]))\n",
    "    mean_y = jnp.array(mu_sigma[dataset_name][\"mean_y\"])\n",
    "    std_y = jnp.sqrt(jnp.array(mu_sigma[dataset_name][\"var_y\"]))\n",
    "    \n",
    "    inputs_test_data = dataset.data[:, dataset.conditional_indices]\n",
    "    outputs_test_data = dataset.data_test[:, dataset.dependent_indices]\n",
    "    inputs_test_data_scaled = (inputs_test_data - mean_x) / std_x\n",
    "    outputs_test_data_scaled = (outputs_test_data - mean_y) / std_y\n",
    "    \n",
    "    sigmas_scaled = jnp.ones_like(sigmas) / std_y\n",
    "    \n",
    "    log_prob_means = computed_lppd_de(\n",
    "        inputs=inputs_test_data_scaled,\n",
    "        outputs=outputs_test_data_scaled,\n",
    "        parameters_network=samples_converted,\n",
    "        parameters_data_std=sigmas_scaled,\n",
    "        regression_model=model,\n",
    "        mean_y=mean_y,\n",
    "        std_y=std_y\n",
    "    )\n",
    "    log_probs_sum = jnp.sum(log_prob_means, axis=0)\n",
    "    log_probs_mean = jnp.mean(log_prob_means, axis=0)\n",
    "    log_probs_std = jnp.std(log_prob_means, axis=0)\n",
    "    de_str = \"{:.2f}, {:.2f}, {:.2f}\".format(log_probs_sum.item(), log_probs_mean.item(), log_probs_std.item())\n",
    "    print(\"de\", dataset_name, de_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "de sinusoidal -60.88, -2.03, 1.40\n",
    "de izmailov -164.60, -2.06, 1.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "de sinusoidal -339.44, -37.72, 22.98\n",
    "de izmailov -612.81, -15.71, 20.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6dd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0d2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1df3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6ad33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method = \"la\"\n",
    "#arch = \"1x3\"\n",
    "dataset_name = \"airfoil\"\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "input_dim = len(dataset.conditional_indices)\n",
    "output_dim = len(dataset.dependent_indices)\n",
    "print(input_dim, output_dim)\n",
    "\n",
    "if arch == \"1x3\":\n",
    "    model_transformation = transformations.Sequential([\n",
    "        nn.Dense(3),\n",
    "        nn.tanh,\n",
    "        nn.Dense(1)\n",
    "    ])\n",
    "    model_transformation_la = torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_dim, 3),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(3, output_dim)\n",
    "    )\n",
    "elif arch == \"3x16\":\n",
    "    model_transformation = transformations.Sequential([\n",
    "        nn.Dense(16),\n",
    "        nn.tanh,\n",
    "        nn.Dense(16),\n",
    "        nn.tanh,\n",
    "        nn.Dense(16),\n",
    "        nn.tanh,\n",
    "        nn.Dense(1)\n",
    "    ])\n",
    "    model_transformation_la = torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_dim, 16),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(16, 16),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(16, 16),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(16, output_dim)\n",
    "    )\n",
    "\n",
    "samples = []\n",
    "sigmas = []\n",
    "\n",
    "file_path_1 = f\"/home/gw/data/experiments/paper/results/weights_{arch}/weights_{method}_mlp_{arch}_{dataset_name}.json\"\n",
    "with open(file_path_1, 'r') as f:\n",
    "    samples_dict = json.load(f)\n",
    "if method == \"la\":\n",
    "    file_path_2 = f\"/home/gw/data/experiments/paper/results/weights_{arch}/sigmas_{method}_mlp_{arch}_{dataset_name}.json\"\n",
    "    with open(file_path_2, 'r') as f:\n",
    "        sigmas_dict = json.load(f)\n",
    "    sigmas = np.ones((len(samples_dict), 1)) * sigmas_dict[\"la_0\"]\n",
    "\n",
    "for key in samples_dict.keys():\n",
    "    sample = samples_dict[key]\n",
    "    samples.append(sample)\n",
    "    if method != \"la\":\n",
    "        sigma = sigmas_dict[key]\n",
    "        sigmas.append(sigma)\n",
    "samples = np.asarray(samples)\n",
    "if method != \"la\":\n",
    "    sigmas = np.asarray(sigmas).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dataset.data[0, dataset.conditional_indices]\n",
    "print(inputs)\n",
    "template = model_transformation.init(rng_key, inputs)\n",
    "samples_converted = []\n",
    "for sample in samples:\n",
    "    sample_converted = conversion.torch_parameters_vector_to_flax_parameters_vector(torch.tensor(sample), template)\n",
    "    samples_converted.append(sample_converted)\n",
    "samples_converted = jnp.array(samples_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = visualization.RegressionFigure(\n",
    "    settings=settings.SettingsRegressionFigure(\n",
    "        settings_plot=settings.SettingsPlot(\n",
    "            alpha=0.2,\n",
    "            aleatoric=False\n",
    "        )\n",
    "    )\n",
    ")\n",
    "figure.plot(\n",
    "    dataset=dataset,\n",
    "    transformation=model_transformation.apply_from_vector,\n",
    "    parameters_list=[samples_converted[:4]],\n",
    "    std=sigmas.mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176de281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Regression(\n",
    "    transformation=model_transformation,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "if method == \"de\":\n",
    "    log_prob_means = computed_lppd_de(\n",
    "        inputs=dataset.data_test[:, dataset.conditional_indices],\n",
    "        outputs=dataset.data_test[:, dataset.dependent_indices],\n",
    "        parameters_network=samples_converted,\n",
    "        parameters_data_std=sigmas,\n",
    "        regression_model=model\n",
    "    )\n",
    "    log_probs_sum = jnp.sum(log_prob_means, axis=0)\n",
    "    log_probs_mean = jnp.mean(log_prob_means, axis=0)\n",
    "    log_probs_std = jnp.std(log_prob_means, axis=0)\n",
    "    print(f\"dataset {dataset_name}, lppd for model: sum {log_probs_sum}, mean {log_probs_mean}, std {log_probs_std}\")\n",
    "    de_str = \"{:.2f}, {:.2f}, {:.2f}\".format(log_probs_sum.item(), log_probs_mean.item(), log_probs_std.item())\n",
    "    print(de_str)\n",
    "elif method == \"la\":\n",
    "    log_prob_means = computed_lppd_la( # USE MCMC formulation here as well\n",
    "        inputs=dataset.data_test[:, dataset.conditional_indices],\n",
    "        outputs=dataset.data_test[:, dataset.dependent_indices],\n",
    "        parameters_network=samples,\n",
    "        parameters_data_std=sigmas,\n",
    "        regression_model=model\n",
    "    )\n",
    "    log_probs_sum = jnp.sum(log_prob_means, axis=0)\n",
    "    log_probs_mean = jnp.mean(log_prob_means, axis=0)\n",
    "    log_probs_std = jnp.std(log_prob_means, axis=0)\n",
    "    print(f\"dataset {dataset_name}, lppd for model: sum {log_probs_sum}, mean {log_probs_mean}, std {log_probs_std}\")\n",
    "    la_str = \"{:.2f}, {:.2f}, {:.2f}\".format(log_probs_sum.item(), log_probs_mean.item(), log_probs_std.item())\n",
    "    print(la_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdaec9",
   "metadata": {},
   "source": [
    "# laplace approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import laplace\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from utils.conversion import flax_parameters_dict_to_torch_parameters_vector, torch_parameters_vector_to_flax_parameters_dict, torch_parameters_vector_to_flax_parameters_vector, torch_to_flax_permutation\n",
    "from numpyro import distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimate_la_vector = samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc147ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self._data[index]\n",
    "        conditional = sample[..., :-1]\n",
    "        dependent = sample[..., -1:]\n",
    "        return conditional, dependent\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "torch_dataset = ConditionalDataset(data=torch.from_numpy(np.array(dataset.data)))\n",
    "train_loader = DataLoader(torch_dataset, batch_size=len(torch_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean = samples_converted[0]\n",
    "posterior_mean_la = samples[0]\n",
    "print(posterior_mean.shape, posterior_mean_la.shape)\n",
    "\n",
    "torch.nn.utils.vector_to_parameters(torch.tensor(posterior_mean_la, dtype=torch.float), model_transformation_la.parameters())\n",
    "\n",
    "# initial la fit\n",
    "hessian=\"diag\"\n",
    "model = model_transformation_la\n",
    "la = laplace.Laplace(\n",
    "    model=model,\n",
    "    sigma_noise=sigmas.mean(),\n",
    "    likelihood=\"regression\",\n",
    "    subset_of_weights=\"all\",\n",
    "    hessian_structure=hessian,\n",
    "    prior_precision=1.0\n",
    ")\n",
    "la.fit(train_loader)\n",
    "\n",
    "# optimize sigma noise and prior precision\n",
    "log_prior, log_sigma = torch.zeros(1, requires_grad=True), torch.zeros(1, requires_grad=True)\n",
    "print(log_prior.exp(), log_sigma.exp())\n",
    "hyper_optimizer = torch.optim.Adam([log_sigma], lr=1e-2)\n",
    "for i in range(int(1e3)):\n",
    "    hyper_optimizer.zero_grad()\n",
    "    neg_marglik = - la.log_marginal_likelihood(log_prior.exp(), log_sigma.exp())\n",
    "    neg_marglik.backward()\n",
    "    hyper_optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            print(log_prior.exp(), log_sigma.exp())\n",
    "print(log_prior.exp(), log_sigma.exp())\n",
    "\n",
    "permutation_indices = torch_to_flax_permutation(template)\n",
    "\n",
    "if hessian == \"diag\":\n",
    "    posterior_precision = jnp.array(np.diag(la.posterior_precision.detach().numpy()))[permutation_indices][:, permutation_indices]\n",
    "else:\n",
    "    posterior_precision = jnp.array(la.posterior_precision.detach().numpy())[permutation_indices][:, permutation_indices]\n",
    "\n",
    "posterior = distributions.MultivariateNormal(posterior_mean, precision_matrix=posterior_precision)\n",
    "#la_samples_custom_0 = posterior.sample(rng_key, (1274, ))\n",
    "la_samples_package = jnp.array(la.sample(n_samples=1274).detach().numpy())\n",
    "la_samples_package_flax = la_samples_package[:, permutation_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b697ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "la_samples = la_samples_package_flax\n",
    "la_sigma = jnp.array(log_sigma.exp().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b34180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(la_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = visualization.RegressionFigure(\n",
    "    settings=settings.SettingsRegressionFigure(\n",
    "        settings_plot=settings.SettingsPlot(\n",
    "            alpha=0.8,\n",
    "            aleatoric=False\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "figure_regression = figure.plot(\n",
    "    dataset=dataset,\n",
    "    transformation=model_transformation.apply_from_vector,\n",
    "    parameters_list=[la_samples.squeeze()[:16]],\n",
    "    std=0.1\n",
    ")\n",
    "figure_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea86920",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Regression(\n",
    "    transformation=model_transformation,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "\n",
    "log_prob_means = computed_lppd_la( # USE MCMC formulation here as well\n",
    "    inputs=dataset.data_test[:, dataset.conditional_indices],\n",
    "    outputs=dataset.data_test[:, dataset.dependent_indices],\n",
    "    parameters_network=la_samples,\n",
    "    parameters_data_std=jnp.ones((len(la_samples), 1)) * la_sigma,\n",
    "    regression_model=model\n",
    ")\n",
    "log_probs_sum = jnp.sum(log_prob_means, axis=0)\n",
    "log_probs_mean = jnp.mean(log_prob_means, axis=0)\n",
    "log_probs_std = jnp.std(log_prob_means, axis=0)\n",
    "print(f\"dataset {dataset_name}, lppd for model: sum {log_probs_sum}, mean {log_probs_mean}, std {log_probs_std}\")\n",
    "la_str = \"{:.2f}, {:.2f}, {:.2f}\".format(log_probs_sum.item(), log_probs_mean.item(), log_probs_std.item())\n",
    "\n",
    "print(\"de\")\n",
    "print(de_str)\n",
    "print(\"la\")\n",
    "print(la_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
